{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (Support Vector Machines)\n",
    "1) A support vector machine (SVM) is a supervised machine learning algorithm used for Classification and Regression<br>\n",
    "2) A SVM takes in takes the data points (inputs) and outputs the hyperplane (which in two dimensions is simply a line) that best separates the data points.<br>\n",
    "3) The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points<br>\n",
    "4) It follows a technique called the <b> kernel trick to transform</b> the data and based on these transformations, it finds an optimal boundary between the possible outputs.<br>\n",
    "5) <b>SVM Intuition - The main idea is to identify the optimal separating hyperplane which maximizes the margin of the training data.</b><br>\n",
    "6) There can be multiple hyperplanes, but which one of them is the best separating hyperplane? It can be easily seen that line B is the one which best separates the two classes.\n",
    "\n",
    "<img src=\"svm1.png\" width=\"450\" height=\"400\">\n",
    "\n",
    "7) There can be multiple separating hyperplanes as well. How do we find the optimal one? Intuitively, if we select a hyperplane which is close to the data points of one class, then it might not generalize well(It may not generate good results for the test data.) <b>So the aim is to choose the hyperplane which is as far as possible from the data points of each category.</b>\n",
    "Therefore, maximizing the distance between the nearest points of each class and the hyperplane would result in an optimal separating hyperplane. This distance is called the margin.\n",
    "The goal of SVMs is to find the optimal hyperplane because it not only classifies the existing dataset but also helps predict the class of the unseen data. And the optimal hyperplane is the one which has the biggest margin.<br>\n",
    "\n",
    "8)\tTo separate the two classes of data points, there are many possible hyperplanes that could be chosen. SVMs objective is to find a plane that has the maximum margin, i.e. the maximum distance between data points of both classes. \n",
    "\n",
    "<img src=\"svm2.png\">\n",
    "<img src=\"svm3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminologies\n",
    "\n",
    "1)<b> Hyperplane </b>- Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.<br>\n",
    "\n",
    "2)<b> Support Vectors </b>- Support vectors are data points that are closest to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, the model maximizes the margin of the classifier. These are the points that help us build our SVM.\n",
    "\n",
    "3)<b> Margin </b>– It is the distance between support vectors and the hyperplane.\n",
    "\n",
    "<img src=\"svm4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "\n",
    "<img src=\"svm_kernel.png\" height=\"350\" width=\"500\">\n",
    "Non-linearly separabale data can be classified using SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Parameters\n",
    "\n",
    "<b>1) Gamma</b> - The gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. <br>\n",
    "Low gamma values emphasises on far away data points from the hyperplane<br>\n",
    "High gamma values emphasises on data points  close to the hyperplane<br>\n",
    "Used with rbf kernel. We take values like 0.001,0.01,0.1,1,10,100,1000 and on on.\n",
    "\n",
    "\n",
    "<b>2)  C </b> The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. Small values of C attribues to more misclassification and larger values of C attribues to less misclassification. <br>\n",
    "We take values like 0.001,0.01,0.1,1,10,100,1000 and so on.\n",
    "\n",
    "<b>3) kernel</b> - SVM follows a technique called the kernel trick to transform the data and based on these transformations, it finds an optimal boundary between the possible outputs. Different kernel options are linear, rbf, poly\n",
    "\n",
    "<b>4) degree</b> - Defines degree of polynomial for 'poly' kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Types\n",
    "\n",
    "<b>1) Linear</b>\n",
    "<b>K(xi, xj) = xi.xj</b>\n",
    "Here, xi, xj represents the data you’re trying to classify.\n",
    "\n",
    "<b>2) Poly</b>\n",
    "<b>K(xi, xj) = (xi.xj+1)^d</b>\n",
    "Here ‘.’ shows the dot product of both the values, and d denotes the degree of the polynomial.\n",
    "K(xi, xj) representing the decision boundary to separate the given classes. \n",
    "\n",
    "<b>3) RBF (Radial Basis Function) </b>\n",
    "<b>K(xi, xj) = exp(-γ*||xi - xj||^2)\n",
    "The value of γ>0\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Pros and Cons\n",
    "#### Pros\n",
    "\n",
    "1) SVM is an algorithm which is suitable for both linearly and nonlinearly separable data (using kernel trick).<br>\n",
    "2) SVM is very good when we have no idea about the data.<br>\n",
    "3) Kernel trick is what makes SVM unique.<br>\n",
    "\n",
    "#### Cons\n",
    "\n",
    "1) They are not suitable for larger datasets because the training time with SVMs can be high and much more computationally intensive.<br>\n",
    "2) Tuning parameters can be time consuming<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    0\n",
       "sepal_width     0\n",
       "petal_length    0\n",
       "petal_width     0\n",
       "label           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    float64\n",
       "sepal_width     float64\n",
       "petal_length    float64\n",
       "petal_width     float64\n",
       "label            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Iris-setosa        49\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 4)\n",
      "(149,)\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[:,:-1]  # x = df.drop('label',axis=1)\n",
    "y = df.iloc[:,-1]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 4)\n",
      "(38, 4)\n",
      "(111,)\n",
      "(38,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying SVM with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # SupportVectorClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = SVC(kernel='linear',C=1)\n",
    "m1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.990990990990991\n",
      "Testing Score 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Training Score',m1.score(x_train,y_train))\n",
    "print('Testing Score',m1.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ypred\n",
      " ['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "ypred_m1 = m1.predict(x_test)\n",
    "print('ypred\\n',ypred_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  1 13]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         8\n",
      "Iris-versicolor       0.94      1.00      0.97        16\n",
      " Iris-virginica       1.00      0.93      0.96        14\n",
      "\n",
      "       accuracy                           0.97        38\n",
      "      macro avg       0.98      0.98      0.98        38\n",
      "   weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_m1 = confusion_matrix(y_test,ypred_m1)\n",
    "print(cm_m1)\n",
    "print(classification_report(y_test,ypred_m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying SVM with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, gamma=0.1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = SVC(kernel='rbf',gamma=0.1,C=0.1)\n",
    "m2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.963963963963964\n",
      "Testing Score 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "print('Training Score',m2.score(x_train,y_train))\n",
    "print('Testing Score',m2.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying SVM with poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, kernel='poly')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = SVC(kernel='poly',degree=3,C=10)\n",
    "m3.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.990990990990991\n",
      "Testing Score 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "print('Training Score',m3.score(x_train,y_train))\n",
    "print('Testing Score',m3.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
